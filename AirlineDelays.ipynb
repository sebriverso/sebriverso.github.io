{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pydotplus\n",
        "!pip install tensorflow_decision_forests"
      ],
      "metadata": {
        "id": "vv21oknD3rjl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "906dac2f-206a-4edc-dd04-50f6c37507e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pydotplus in /usr/local/lib/python3.10/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from pydotplus) (3.0.9)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_decision_forests\n",
            "  Downloading tensorflow_decision_forests-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow_decision_forests) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorflow_decision_forests) (1.22.4)\n",
            "Requirement already satisfied: tensorflow~=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_decision_forests) (2.12.0)\n",
            "Collecting wurlitzer\n",
            "  Downloading wurlitzer-3.0.3-py3-none-any.whl (7.3 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from tensorflow_decision_forests) (1.5.3)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from tensorflow_decision_forests) (0.40.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tensorflow_decision_forests) (1.16.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tensorflow_decision_forests) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tensorflow_decision_forests) (3.20.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tensorflow_decision_forests) (0.32.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tensorflow_decision_forests) (16.0.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tensorflow_decision_forests) (1.6.3)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tensorflow_decision_forests) (2.12.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tensorflow_decision_forests) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tensorflow_decision_forests) (67.7.2)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tensorflow_decision_forests) (23.3.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tensorflow_decision_forests) (2.12.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tensorflow_decision_forests) (23.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tensorflow_decision_forests) (2.3.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tensorflow_decision_forests) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tensorflow_decision_forests) (1.54.0)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tensorflow_decision_forests) (2.12.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tensorflow_decision_forests) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tensorflow_decision_forests) (0.2.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tensorflow_decision_forests) (0.4.8)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.12.0->tensorflow_decision_forests) (3.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow_decision_forests) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow_decision_forests) (2022.7.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow~=2.12.0->tensorflow_decision_forests) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow~=2.12.0->tensorflow_decision_forests) (1.10.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tensorflow_decision_forests) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tensorflow_decision_forests) (0.7.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tensorflow_decision_forests) (2.3.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tensorflow_decision_forests) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tensorflow_decision_forests) (1.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tensorflow_decision_forests) (2.27.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tensorflow_decision_forests) (2.17.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tensorflow_decision_forests) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tensorflow_decision_forests) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tensorflow_decision_forests) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tensorflow_decision_forests) (1.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tensorflow_decision_forests) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tensorflow_decision_forests) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tensorflow_decision_forests) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tensorflow_decision_forests) (1.26.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tensorflow_decision_forests) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tensorflow_decision_forests) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tensorflow_decision_forests) (3.2.2)\n",
            "Installing collected packages: wurlitzer, tensorflow_decision_forests\n",
            "Successfully installed tensorflow_decision_forests-1.3.0 wurlitzer-3.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMLRj85HS-Jk",
        "outputId": "e1811119-c9dc-4430-c36c-15397f5faf96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "# install the dependencies\n",
        "# Python ≥3.5 is required\n",
        "\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "#assert tf.__version__ >= \"2.0\"\n",
        "from tensorflow import keras\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "import time\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Ignore useless warnings (see SciPy issue #5998)\n",
        "import warnings\n",
        "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n",
        "import logging\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # FATAL\n",
        "logging.getLogger('tensorflow').setLevel(logging.FATAL)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the dataset\n",
        "dataset_url = \"https://raw.githubusercontent.com/lee-anh/FlightPredictions/main/flightsResult1.csv\"\n",
        "dataset_url1 =  \"https://raw.githubusercontent.com/lee-anh/FlightPredictions/main/flightsResult2.csv\"\n",
        "df = pd.read_csv(dataset_url)\n",
        "df = df.append(pd.read_csv(dataset_url1))\n",
        "\n",
        "# look at the headers\n",
        "data_top = df.head() \n",
        "data_top"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "Ixx21jUQRpLN",
        "outputId": "a5b56d99-0e37-4dfe-c236-c6323dc94dd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-84f94f07ecbf>:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(pd.read_csv(dataset_url1))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0   AWND  PRCP  SNOW  SNWD  TAVG  TMAX  TMIN  YEAR  MONTH  ...  \\\n",
              "0           0  14.54   0.0   0.0   0.0    20    32    14  2015      1  ...   \n",
              "1           1  14.54   0.0   0.0   0.0    20    32    14  2015      1  ...   \n",
              "2           2  14.54   0.0   0.0   0.0    20    32    14  2015      1  ...   \n",
              "3           3  14.54   0.0   0.0   0.0    20    32    14  2015      1  ...   \n",
              "4           4  14.54   0.0   0.0   0.0    20    32    14  2015      1  ...   \n",
              "\n",
              "   AIRLINE  FLIGHT_NUMBER TAIL_NUMBER  ORIGIN_AIRPORT DESTINATION_AIRPORT  \\\n",
              "0       US            602      N197UW             ORD                 PHX   \n",
              "1       UA           1500      N30401             ORD                 IAH   \n",
              "2       NK            409      N509NK             ORD                 FLL   \n",
              "3       UA           1167      N66837             ORD                 DEN   \n",
              "4       EV           5498      N132EV             ORD                 DTW   \n",
              "\n",
              "  SCHEDULED_DEPARTURE DEPARTURE_DELAY  SCHEDULED_TIME  DISTANCE  \\\n",
              "0                 500            -1.0           228.0      1440   \n",
              "1                 510             4.0           175.0       925   \n",
              "2                 530            -4.0           188.0      1182   \n",
              "3                 533             7.0           167.0       888   \n",
              "4                 535            15.0            85.0       235   \n",
              "\n",
              "   SCHEDULED_ARRIVAL  \n",
              "0                748  \n",
              "1                805  \n",
              "2                938  \n",
              "3                720  \n",
              "4                800  \n",
              "\n",
              "[5 rows x 22 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-729400eb-57cc-4e5d-b60c-cc3ca66d30a5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>AWND</th>\n",
              "      <th>PRCP</th>\n",
              "      <th>SNOW</th>\n",
              "      <th>SNWD</th>\n",
              "      <th>TAVG</th>\n",
              "      <th>TMAX</th>\n",
              "      <th>TMIN</th>\n",
              "      <th>YEAR</th>\n",
              "      <th>MONTH</th>\n",
              "      <th>...</th>\n",
              "      <th>AIRLINE</th>\n",
              "      <th>FLIGHT_NUMBER</th>\n",
              "      <th>TAIL_NUMBER</th>\n",
              "      <th>ORIGIN_AIRPORT</th>\n",
              "      <th>DESTINATION_AIRPORT</th>\n",
              "      <th>SCHEDULED_DEPARTURE</th>\n",
              "      <th>DEPARTURE_DELAY</th>\n",
              "      <th>SCHEDULED_TIME</th>\n",
              "      <th>DISTANCE</th>\n",
              "      <th>SCHEDULED_ARRIVAL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>14.54</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20</td>\n",
              "      <td>32</td>\n",
              "      <td>14</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>US</td>\n",
              "      <td>602</td>\n",
              "      <td>N197UW</td>\n",
              "      <td>ORD</td>\n",
              "      <td>PHX</td>\n",
              "      <td>500</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>1440</td>\n",
              "      <td>748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>14.54</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20</td>\n",
              "      <td>32</td>\n",
              "      <td>14</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>UA</td>\n",
              "      <td>1500</td>\n",
              "      <td>N30401</td>\n",
              "      <td>ORD</td>\n",
              "      <td>IAH</td>\n",
              "      <td>510</td>\n",
              "      <td>4.0</td>\n",
              "      <td>175.0</td>\n",
              "      <td>925</td>\n",
              "      <td>805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>14.54</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20</td>\n",
              "      <td>32</td>\n",
              "      <td>14</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>NK</td>\n",
              "      <td>409</td>\n",
              "      <td>N509NK</td>\n",
              "      <td>ORD</td>\n",
              "      <td>FLL</td>\n",
              "      <td>530</td>\n",
              "      <td>-4.0</td>\n",
              "      <td>188.0</td>\n",
              "      <td>1182</td>\n",
              "      <td>938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>14.54</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20</td>\n",
              "      <td>32</td>\n",
              "      <td>14</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>UA</td>\n",
              "      <td>1167</td>\n",
              "      <td>N66837</td>\n",
              "      <td>ORD</td>\n",
              "      <td>DEN</td>\n",
              "      <td>533</td>\n",
              "      <td>7.0</td>\n",
              "      <td>167.0</td>\n",
              "      <td>888</td>\n",
              "      <td>720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>14.54</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20</td>\n",
              "      <td>32</td>\n",
              "      <td>14</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>EV</td>\n",
              "      <td>5498</td>\n",
              "      <td>N132EV</td>\n",
              "      <td>ORD</td>\n",
              "      <td>DTW</td>\n",
              "      <td>535</td>\n",
              "      <td>15.0</td>\n",
              "      <td>85.0</td>\n",
              "      <td>235</td>\n",
              "      <td>800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 22 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-729400eb-57cc-4e5d-b60c-cc3ca66d30a5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-729400eb-57cc-4e5d-b60c-cc3ca66d30a5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-729400eb-57cc-4e5d-b60c-cc3ca66d30a5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "# data preprocessing\n",
        "\n",
        "# ONE HOT ENCODING TUTORIAL HERE https://www.geeksforgeeks.org/ml-one-hot-encoding-of-datasets-in-python/\n",
        "# ADDS EXTRA COLUMN FOR NUMBER CORRESPONDING TO ONE HOT ENCODING WILL DROP THIS\n",
        "# these two make it like 83 \n",
        "df['Condition'] = df['Condition'].astype('category')\n",
        "df['DEST'] = df['DEST'].astype('category')\n",
        "\n",
        "# these two one hots make it 2k \n",
        "df['OP_UNIQUE_CARRIER'] = df['OP_UNIQUE_CARRIER'].astype('category')\n",
        "# df['TAIL_NUM'] = df['TAIL_NUM'].astype('category')\n",
        "  \n",
        "  \n",
        "# Assigning numerical values and storing it in another columns\n",
        "df['Gen_new'] = df['Condition'].cat.codes\n",
        "df['Rem_new'] = df['DEST'].cat.codes\n",
        "\n",
        "df['Car_new'] = df['OP_UNIQUE_CARRIER'].cat.codes\n",
        "  \n",
        "  \n",
        "# Create an instance of One-hot-encoder\n",
        "enc = OneHotEncoder()\n",
        "  \n",
        "# Passing encoded columns\n",
        "enc_data = pd.DataFrame(enc.fit_transform(\n",
        "    df[['Gen_new', 'Rem_new','Car_new']]).toarray())\n",
        "  \n",
        "# Merge with main\n",
        "New_df = df.join(enc_data)\n",
        "\n",
        "# dropping extra columns\n",
        "New_df = New_df.drop(\"Gen_new\", axis='columns')\n",
        "New_df = New_df.drop(\"Rem_new\", axis='columns')\n",
        "New_df = New_df.drop(\"Car_new\", axis='columns')\n",
        "New_df = New_df.drop(\"Condition\", axis='columns')\n",
        "New_df = New_df.drop(\"DEST\", axis='columns')\n",
        "New_df = New_df.drop(\"OP_UNIQUE_CARRIER\", axis='columns')\n",
        "New_df = New_df.drop(\"Wind\", axis='columns')\n",
        "\n",
        "# TODO: group planes by number of delays\n",
        "New_df = New_df.drop(\"TAIL_NUM\", axis='columns')\n",
        "\n",
        "\n",
        "\n",
        "# scaling pressure by 100 so that the floats don't get truncated when we convert to ints\n",
        "\n",
        "# I think we need to convert all non integer types into integers\n",
        "# meow meow meow \n",
        "carrier_cats = sorted(df[\"OP_UNIQUE_CARRIER\"].unique())\n",
        "dest_cats = sorted(df[\"DEST\"].unique())\n",
        "wind_cats = sorted(df[\"Wind\"].astype(str).unique()) # there is a nan value, we could remove that data point\n",
        "condition_cats = sorted(df[\"Condition\"].unique())\n",
        "\n",
        "# convert to dictionaries ... we might not need this anymore \n",
        "carrier_dict = dict(enumerate(carrier_cats)) \n",
        "carrier_dict = {y: x for x, y in carrier_dict.items()} # flip the keys and values \n",
        "dest_dict = dict(enumerate(dest_cats)) \n",
        "dest_dict = {y: x for x, y in dest_dict.items()}\n",
        "wind_dict = dict(enumerate(wind_cats)) \n",
        "wind_dict = {y: x for x, y in wind_dict.items()}\n",
        "condition_dict = dict(enumerate(condition_cats)) \n",
        "condition_dict = {y: x for x, y in condition_dict.items()}\n",
        "\n",
        "\n",
        "# apply the maps to convert the data\n",
        "vocab = carrier_cats + dest_cats + wind_cats + condition_cats\n",
        "assert len(vocab) == len(set(vocab)) # cool we have a unqiue string vocab  \n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "IE56ciS9S4pM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "c563086d-bb68-4c03-f576-a962ebabeebd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# I think we need to convert all non integer types into integers\\n# meow meow meow \\ncarrier_cats = sorted(df[\"OP_UNIQUE_CARRIER\"].unique())\\ndest_cats = sorted(df[\"DEST\"].unique())\\nwind_cats = sorted(df[\"Wind\"].astype(str).unique()) # there is a nan value, we could remove that data point\\ncondition_cats = sorted(df[\"Condition\"].unique())\\n\\n# convert to dictionaries ... we might not need this anymore \\ncarrier_dict = dict(enumerate(carrier_cats)) \\ncarrier_dict = {y: x for x, y in carrier_dict.items()} # flip the keys and values \\ndest_dict = dict(enumerate(dest_cats)) \\ndest_dict = {y: x for x, y in dest_dict.items()}\\nwind_dict = dict(enumerate(wind_cats)) \\nwind_dict = {y: x for x, y in wind_dict.items()}\\ncondition_dict = dict(enumerate(condition_cats)) \\ncondition_dict = {y: x for x, y in condition_dict.items()}\\n\\n\\n# apply the maps to convert the data\\nvocab = carrier_cats + dest_cats + wind_cats + condition_cats\\nassert len(vocab) == len(set(vocab)) # cool we have a unqiue string vocab  \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WN85XYRtAocV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y \n",
        "import tensorflow_decision_forests as tfdf\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix \n",
        "from sklearn.metrics import accuracy_score \n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.tree import export_graphviz\n",
        "from sklearn import tree\n",
        "from six import StringIO\n",
        "import pydotplus\n",
        "from IPython.display import Image \n",
        "dot_data = StringIO()\n",
        "from sklearn import tree\n",
        "import xgboost as xgb\n",
        "\n",
        "\n",
        "#importing the data\n",
        "\n",
        "\n",
        "\n",
        "df.columns = df.columns.astype(str)\n",
        "\n",
        "delay_times = df.loc[:, \"DEPARTURE_DELAY\"]\n",
        "df = df.drop([\"DEPARTURE_DELAY\"], axis=\"columns\")\n",
        "number_of_classes = 1 \n",
        "y = delay_times.map(lambda a : 1 if a > 0 else 0 ) # using a binary classifer right now, but will change \n",
        "\n",
        "print(y)\n",
        "\n",
        "\n",
        "# drop features that are unimportant / would potentially confound the data\n",
        "\n",
        "\n",
        "# X\n",
        "X = df\n",
        "number_of_features = df.shape[1]\n",
        "\n",
        "\n",
        "# split the dataset to train, validation, and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.25, random_state = 1)\n",
        "\n",
        "# simple binary classifer\n",
        "\n",
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Changing data to int\n",
        "\n",
        "#X_train=np.asarray(X_train).astype(int)\n",
        "#y_train=np.asarray(y_train).astype(int)\n",
        "\n",
        "#X_val=np.asarray(X_val).astype(int)\n",
        "#y_val=np.asarray(y_val).astype(int)\n",
        "\n",
        "\"\"\"\n",
        "#DECISION TREE MODEL\n",
        "#Decision tree model, does the best out of everything, but can be made better\n",
        "clf = XGBClassifier(random_state = 0)\n",
        "clf.fit(X_train, y_train) \n",
        "\n",
        "tree.export_graphviz(clf, max_depth = 3, feature_names=X_test.columns, out_file= dot_data)\n",
        "graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) \n",
        "graph.write_png('tree.png')\n",
        "# image stored in files\n",
        "Image(graph.create_png())\n",
        "\n",
        "\n",
        "\n",
        "print(X_test)\n",
        "y_predict = clf.predict(X_test)\n",
        "cmatrix = confusion_matrix(y_test, y_predict) \n",
        "accuracy = accuracy_score(y_test, y_predict) \n",
        "classification_report = classification_report(y_test, y_predict) \n",
        "\n",
        "print(cmatrix)\n",
        "print(classification_report)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#tf_dataset = tfdf.keras.pd_dataframe_to_tf_dataset(df, label=\"DEPARTURE_DELAY\")\n",
        "\n",
        "tuner = tfdf.tuner.RandomSearch(num_trials=20)\n",
        "\n",
        "# Hyper-parameters to optimize.\n",
        "#tuner.discret(\"max_depth\", [4, 5, 6, 7])\n",
        "\n",
        "model = tfdf.keras.GradientBoostedTreesModel() #tuner=tuner\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "\n",
        "\n",
        "# we could try different things like logisitic, then softmax, then cnn and then with cnn we can do hyper parameter tuning "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "eWBOCLZ9TxXk",
        "outputId": "d0d0e0d5-8141-44fd-fe4f-346d938b1b7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0         0\n",
            "1         1\n",
            "2         0\n",
            "3         1\n",
            "4         1\n",
            "         ..\n",
            "145879    0\n",
            "145880    1\n",
            "145881    0\n",
            "145882    0\n",
            "145883    1\n",
            "Name: DEPARTURE_DELAY, Length: 285884, dtype: int64\n",
            "Use /tmp/tmp85b95oof as temporary training directory\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-d9f8a59e90fa>\u001b[0m in \u001b[0;36m<cell line: 91>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientBoostedTreesModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#tuner=tuner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_decision_forests/keras/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, callbacks, verbose, validation_steps, validation_data, sample_weight, steps_per_epoch, class_weight, **kwargs)\u001b[0m\n\u001b[1;32m   1200\u001b[0m     \u001b[0;31m# Check for a Pandas Dataframe without injecting a dependency.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"<class 'pandas.core.frame.DataFrame'>\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1202\u001b[0;31m       raise ValueError(\n\u001b[0m\u001b[1;32m   1203\u001b[0m           \u001b[0;34m\"`fit` cannot consume Pandas' dataframes directly. Instead, use the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m           \u001b[0;34m\"`pd_dataframe_to_tf_dataset` utility function. For example: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: `fit` cannot consume Pandas' dataframes directly. Instead, use the `pd_dataframe_to_tf_dataset` utility function. For example: `model.fit(tfdf.keras.pd_dataframe_to_tf_dataset(train_dataframe, label=\"label_column\"))"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Decision Tree has excellent performance. However, Decision Trees are unstable and for this model to be generalizable to other airports we will need to use a more stable model, like a Neural Network. "
      ],
      "metadata": {
        "id": "tcJmHp5sULoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DNN MODEL as a binary classifier \n",
        "\n",
        "simple_model = keras.models.Sequential()\n",
        "\n",
        "simple_model.add(keras.layers.Dense(100, activation = 'tanh'))\n",
        "simple_model.add(keras.layers.Dense(50, activation = 'tanh'))\n",
        "simple_model.add(keras.layers.Dense(15, activation = 'tanh'))\n",
        "simple_model.add(keras.layers.Dense(5, activation = 'relu'))\n",
        "simple_model.add(keras.layers.Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "simple_model.compile(optimizer='adam', loss='binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "\n",
        "simple_model.summary\n",
        "simple_model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jSNCbEvOZSYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our DNN has low variance but high bias which means that the model is underfitting. We will try to use a more complex model. "
      ],
      "metadata": {
        "id": "eKPpNEYFRzLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model 2 DNN\n",
        "\n",
        "\n",
        "model_2 = keras.models.Sequential()\n",
        "for x in range(500): \n",
        "    model_2.add(keras.layers.Dense(256, activation = 'relu'))\n",
        "\n",
        "\n",
        "model_2.add(keras.layers.Dense(5, activation = \"relu\"))\n",
        "model_2.add(keras.layers.Dense(1, activation = 'sigmoid'))\n",
        "model_2.compile(optimizer='adam', loss='binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "\n",
        "model_2.summary\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "tic = time.process_time()\n",
        "model_2_history = model_2.fit(X_train, y_train, epochs=num_epochs, validation_data=(X_val, y_val))\n",
        "toc = time.process_time()\n",
        "\n",
        "\n",
        "#Plot performnace\n",
        "pd.DataFrame(model_2_history.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1)\n",
        "plt.show()\n",
        "\n",
        "time_nn_f=toc-tic\n",
        "print(\"Time to train the model: {0:.4f} secs \".format((time_nn_f)))\n",
        "print(\"Training Set Accuracy:   {0:.4%} & Loss:{1:.4f}\".format(model_2_history.history['accuracy'][num_epochs-1],model_2_history.history['loss'][num_epochs-1]))\n",
        "print(\"Validation Set Accuracy: {0:.4%} & Loss:{1:.4f}\".format(model_2_history.history['val_accuracy'][num_epochs-1],model_2_history.history['val_loss'][num_epochs-1]))\n",
        "\n",
        "\n",
        "# even when we add a bunchhhhh of layers we are underfitting... time to look for better features \n"
      ],
      "metadata": {
        "id": "9b3Q2W4ae2RU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We might need to improve our data to have more features"
      ],
      "metadata": {
        "id": "Sx9gLTG3kXxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# now trying with xgboost "
      ],
      "metadata": {
        "id": "5Q4ftzC2Aw0M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}